\chapter{Introduction}
\label{sec:intro}

The amount of multimedia data currently available is immense and is growing at an unprecedented rate. For instance, more than five hundred hours of video are uploaded every minute on YouTube. Detailed metadata of multimedia data, such as music, video, and images, is typically not available because of the cost and the hard task of meaningfully describing these objects. This problem can be addressed by representing multimedia objects using high-dimensional feature vectors or descriptors (100-1000+ dimensions) automatically computed~\cite{Wan:2014:DLC:2647868.2654948,10.1007/978-3-319-10584-0_26,Douze:2009:EGD:1646396.1646421}. 

Searching in these databases, such as performed by content-based multimedia retrieval (CBMR) services, is a fundamental operation for several multimedia applications~\cite{Bohm2001322}. Although similarity search may involve several steps, the most time consuming consists in finding the k nearest neighbors (k-NN) of a query object descriptor(s) in the database. 

The exhaustive exact search is costly due to, not only the large databases currently used by web applications, but also due to the high dimensionality of the descriptors. An alternative to the exhaustive search would be to employ data structures, such as kd-trees~\cite{Friedman1977209} and k-means trees~\cite{Muja2009331}, to partition the space and efficiently prune data partitions during the search. However, the pruning efficiency and, consequently, the performance of these techniques, degrade as the data dimensionality grows due to the well known curse of dimensionality~\cite{Weber:1998:QAP:645924.671192}. 

The approximate nearest neighbors (ANN) search has been proposed as a solution for those applications in which the exact search is not strictly necessary, allowing for accuracy to be traded off for speed. Thus, a large number of ANN algorithms have been developed~\cite{Gionis:1999:SSH:645925.671516,5432202,NIPS2009_3864,6809191,Muja2009331}. The product quantization nearest neighbor search (also known as IVFADC)~\cite{5432202} has received special attention among competitors due to its ability to minimize memory requirements, while improving speed. This is attained by representing descriptors with small quantization codes and by using an inverted list to avoid exhaustive search in the quantized space. It was recently ported for GPU~\cite{8733051}.

Most of the previous work, however, has focused on optimizing those algorithms for batch execution using sequential settings or a single machine. This decision does not match the demands of modern online CBMR services, which must handle very large and increasing databases that would not fit in the memory of a single machine. Also, while online applications are concerned with minimizing the query response times of individual queries ($query\ response\ time\ =\ queue\ waiting\ time + processing\ time$), the aforementioned algorithms were developed to maximize throughput in a batch execution where several queries are bundled and processed together as a single task. The online scenario presents additional challenges because these systems experience fluctuating workloads, and must adapt to them at run-time to minimize response times. 

In our setting the adaptation is performed in the number of queries (query block size) dispatched for execution with the GPU in each device call. For instance, when the input query rates are low, the queue waiting
time is negligible, and minimizing the processing time is the best approach. This is attained by reducing the query block size
so that more computing resources or core in the GPU are assigned to process each query, thus reducing the processing time of that block of queries. However, as the system load increases, the queue waiting time grows and the system should be configured to improve
the throughput to minimize congesting in queue of queries ready to execute. This is attained by increasing the query block size or number of queries concurrently executed with the GPU. The larger number of queries processed executed concurrently 
improves the throughput because of the reduction on synchronization overheads in a query computation (smaller number of
cores assigned to it) and the best amortized communication and startup overheads.

%few queries can be bundled for GPU execution and, consequently, more resource %is available to process each of them. When the load increases, however, the %system throughput must be improved to avoid congestion in the queue of queries %waiting to be executed. This is attained by bundling a larger number of %queries for concurrent execution with the GPU, which will attain higher %efficiency because of the lower synchronization costs due to the smaller %number of GPU cores assigned to a query and because of the amortized costs of %launching the computation in the GPU. Dealing with this variation is %fundamental for delivering the best response times in all scenarios.

This work addresses these challenges with a distributed-memory parallelization of IVFADC targeting hybrid machines equipped with CPUs and GPUs. The proposed parallelization avoids data replication and allows for large databases to be searched. Besides, a carefully designed asynchronous communication is performed to minimize overheads. We have also developed multiple strategies to execute query searches using CPU and GPU cooperatively. Even though the GPU can typically reach much higher query processing rates, the CPU is able to significantly reduce the query response times under several query load configurations. We also proposed an out-of-core GPU execution in which the GPU is efficiently used  
when the database index does not fit into its memory. In this setting, the GPU may also be used in cooperation with the CPU, and work stealing is employed to minimize the load imbalance among the devices. 

This paper makes the following major contributions:

\begin{itemize}
    \item We implement an efficient distributed memory version of IVFADC for hybrid CPU-GPU systems. The execution on a CPU-GPU machine can answer 6364 queries/sec on a single node, and its distributed execution scaled with an efficiency of YYY on ZZZ GPUs-CPUs.
    
    \item We have developed the DQPP strategy for adjusting the system at run-time under fluctuating workloads, which decides the number of queries for concurrent execution with the GPU according with the system load. DQPP improved response times vs. the best static approach in 1.29$\times$ and 1.35$\times$, respectively, for the GPU-only and CPU-GPU configurations with moderate loads.
    
    \item We implemented an out-of-core GPU execution scheme that uses work-stealing for maximizing performance on the CPU-GPU cooperative. The CPU-GPU execution with work stealing improved the throughput and response times vs. the GPU-only execution, respectively, in 1.26$\times$ and 2.4$\times$. 
    
\end{itemize}

The remainder of this manuscript is organized as follows: Section~\ref{sec:back} presents a background on high-dimensional similarity search. Section~\ref{sec:pqnns} details the product quantization nearest neighbor search. Section~\ref{sec:parallel} presents the IVFADC parallelization on distributed memory and hybrid CPU-GPU equipped machines and details optimizations to deal with out-of-core GPU execution. The response time aspects are further presented in Section~\ref{sec:query-rate-aware}. Finally, the experimental results are discussed in Section~\ref{sec:experimental-results}, and our conclusions are in Section~\ref{sec:conclusions}.